{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Building Simple Neural Network**"
      ],
      "metadata": {
        "id": "gADC6Yk39WO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Numpy**"
      ],
      "metadata": {
        "id": "NQzZilsp9gtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Sigmoid derivative function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Initialize parameters\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "\n",
        "# Randomly initialize weights and biases\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Forward pass\n",
        "def forward(X):\n",
        "    hidden = sigmoid(np.dot(X, W1) + b1)\n",
        "    output = sigmoid(np.dot(hidden, W2) + b2)\n",
        "    return hidden, output\n",
        "\n",
        "# Backward pass\n",
        "def backward(X, y, hidden, output, learning_rate=0.1):\n",
        "    output_error = y - output\n",
        "    d_output = output_error * sigmoid_derivative(output)\n",
        "\n",
        "    hidden_error = d_output.dot(W2.T)\n",
        "    d_hidden = hidden_error * sigmoid_derivative(hidden)\n",
        "\n",
        "    # Update weights and biases\n",
        "    global W1, b1, W2, b2\n",
        "    W2 += hidden.T.dot(d_output) * learning_rate\n",
        "    b2 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
        "    W1 += X.T.dot(d_hidden) * learning_rate\n",
        "    b1 += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10000):\n",
        "    hidden, output = forward(X)\n",
        "    backward(X, y, hidden, output)\n",
        "\n",
        "# Print the predicted output\n",
        "print(\"Predicted Output:\", forward(X)[1])"
      ],
      "metadata": {
        "id": "hcDXX4xA9jnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Keras**"
      ],
      "metadata": {
        "id": "TSPvut8--UCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Example dataset (XOR problem)\n",
        "X = np.array([[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=3, activation='sigmoid'))  # Hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=10000, verbose=0)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "print(\"Predicted Output:\", predictions)"
      ],
      "metadata": {
        "id": "z3tv_Nq--UOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building LLM from Scratch**"
      ],
      "metadata": {
        "id": "zzHVPWtd_5pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized script**"
      ],
      "metadata": {
        "id": "mjbVkE0tANX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load your custom dataset (replace with actual dataset)\n",
        "dataset = load_dataset(\"your_dataset_here\")\n",
        "train_text = dataset[\"train\"][\"text\"]\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"your_pretrained_tokenizer_here\")\n",
        "tokenized_data = tokenizer(train_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Initialize model\n",
        "model = AutoModelForCausalLM.from_config(\"your_model_config_here\")\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./pretrained_model\",  # where to save the model\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Trainer Setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"input_ids\"],\n",
        ")\n",
        "\n",
        "# Start Pretraining\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZgmvTfwHACmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-Tuning LLM using PEFT-LORA**"
      ],
      "metadata": {
        "id": "QfYZjDA3AgBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized script**"
      ],
      "metadata": {
        "id": "nnRxwdbiAYvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoRA\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Load your custom dataset (replace with actual dataset)\n",
        "dataset = load_dataset(\"your_dataset_here\")\n",
        "train_text = dataset[\"train\"][\"text\"]\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"your_pretrained_tokenizer_here\")\n",
        "tokenized_data = tokenizer(train_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Load model with LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\"your_pretrained_model_here\")\n",
        "model = LoRA(model, rank=8)  # Adjust rank as necessary\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_finetuned_model\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "# Trainer Setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"input_ids\"],\n",
        ")\n",
        "\n",
        "# Start Fine-tuning\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Vgo2nE7cAeg0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}